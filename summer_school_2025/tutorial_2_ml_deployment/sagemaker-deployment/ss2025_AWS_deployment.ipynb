{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "739df786-dc00-4406-9f87-6b04dde97490",
   "metadata": {},
   "source": [
    "# Cloud deployment with AWS SageMaker\n",
    "This notebook showcases how to will **deploy a sentiment analysis model with AWS SageMaker** using serialized pre-trained models. <br> \n",
    "- A serialized sklearn TF-IDF vectorizer is used to compute word embeddings \n",
    "- A serialized sklearn Logistic Regression to predict polarity (positive or negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c8fdd01-2b21-4dd7-803b-418dcd1531ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06983b8c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from nltk) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from nltk) (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85fc626-16a0-4444-8279-db8f01cf4def",
   "metadata": {},
   "source": [
    "# 1. Retrieve model artifacts\n",
    "\n",
    "We will download the pre-trained models from the `ss_deploy_2024` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e64a63f-bdf1-4585-96cb-73f8e68b916f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-07-09 10:18:35--  https://raw.githubusercontent.com/laudavid/ss2024_deploy_app/main/streamlit-app/saved_models/logistic_regression.sav\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4527 (4.4K) [application/octet-stream]\n",
      "Saving to: â€˜logistic_regression.savâ€™\n",
      "\n",
      "100%[======================================>] 4,527       --.-K/s   in 0.001s  \n",
      "\n",
      "2025-07-09 10:18:36 (4.24 MB/s) - â€˜logistic_regression.savâ€™ saved [4527/4527]\n",
      "\n",
      "--2025-07-09 10:18:36--  https://raw.githubusercontent.com/laudavid/ss2024_deploy_app/main/streamlit-app/saved_models/tfidf-vectorizer.sav\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 940273 (918K) [application/octet-stream]\n",
      "Saving to: â€˜tfidf-vectorizer.savâ€™\n",
      "\n",
      "100%[======================================>] 940,273     --.-K/s   in 0.01s   \n",
      "\n",
      "2025-07-09 10:18:36 (69.5 MB/s) - â€˜tfidf-vectorizer.savâ€™ saved [940273/940273]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download pre-trained model and pre-processing function \n",
    "!wget https://raw.githubusercontent.com/laudavid/ss2024_deploy_app/main/streamlit-app/saved_models/logistic_regression.sav # logistic regression model\n",
    "!wget https://raw.githubusercontent.com/laudavid/ss2024_deploy_app/main/streamlit-app/saved_models/tfidf-vectorizer.sav # tf-idf vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6259fe34",
   "metadata": {},
   "source": [
    "# 2. Build an inference script\n",
    "\n",
    "AWS Sagemaker requires an **inference script** to load the pre-trained model and run predictions. <br>\n",
    "This script contains the following elements:\n",
    "-  `model_fn()`: A function that loads the pre-trained models \n",
    "-  `predict_fn()`: A function that generates predictions. It includes steps to clean the input data, apply the tf-idf vectorizer and generate predictions with a logistic regression model.\n",
    "- `input_fn()`/`output_fn()`: Functions for input and output request processing.\n",
    "\n",
    "*Note: You should always use these function names since SageMaker expects these specific functions to exist when deploying models.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be181958",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile inference.py\n",
    "\n",
    "import os\n",
    "import json\n",
    "import joblib \n",
    "from preprocess import text_preprocessing\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \"\"\"\n",
    "    Load pre-trained models\n",
    "    \"\"\"\n",
    "    model = joblib.load(os.path.join(model_dir, 'logistic_regression.sav'))\n",
    "    tfidf = joblib.load(os.path.join(model_dir, 'tfidf-vectorizer.sav'))\n",
    "    model_dict = {\"vectorizer\":tfidf, \"model\":model}\n",
    "    \n",
    "    return model_dict\n",
    "\n",
    "\n",
    "def predict_fn(input_data, model):\n",
    "    \"\"\"\n",
    "    Apply text vectorizer and model to the incoming request\n",
    "    \"\"\"\n",
    "    tfidf = model['vectorizer']\n",
    "    lr_model = model['model']\n",
    "        \n",
    "    clean_text = text_preprocessing(input_data)\n",
    "    embedding = tfidf.transform(clean_text)\n",
    "    prediction = lr_model.predict(embedding)\n",
    "\n",
    "    return prediction.tolist()\n",
    "\n",
    "\n",
    "def input_fn(request_body, request_content_type):\n",
    "    \"\"\"\n",
    "    Deserialize and prepare the prediction input\n",
    "    \"\"\"\n",
    "\n",
    "    if request_content_type == \"application/json\":\n",
    "        request = json.loads(request_body)\n",
    "    else:\n",
    "        request = request_body\n",
    "\n",
    "    return request\n",
    "\n",
    "\n",
    "def output_fn(prediction, response_content_type):\n",
    "    \"\"\"\n",
    "    Serialize and prepare the prediction output\n",
    "    \"\"\"\n",
    "\n",
    "    if response_content_type == \"application/json\":\n",
    "        response = json.dumps(prediction)\n",
    "    else:\n",
    "        response = str(prediction) \n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612bcc3b",
   "metadata": {},
   "source": [
    "# 3. Create a requirements.txt file\n",
    "\n",
    "The `requirements.txt` file allows SageMaker to install the packaged needed for your model to run. <br>\n",
    "In our case, we added `nltk`as the only external library to install.\n",
    "\n",
    "*Note: Some third-party libraries such as scikit-learn or pandas are pre-installed in SageMaker and don't need to be added to this file.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85069434",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt \n",
    "\n",
    "nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad253353",
   "metadata": {},
   "source": [
    "# 4. Package and upload to S3\n",
    "SageMaker requires that the deployment package be structured in a compatible format. <br> It expects all files to be packaged in a tar archive named **\"model.tar.gz\"** with gzip compression \n",
    "\n",
    "We are going to package the pre-trained models, preprocessing and inferencing scripts, as well as the requirements.txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c934535",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic_regression.sav\n",
      "tfidf-vectorizer.sav\n",
      "preprocess.py\n",
      "inference.py\n",
      "requirements.txt\n"
     ]
    }
   ],
   "source": [
    "# Create a tarball with the models, scripts, and requirements\n",
    "!tar -cvpzf model.tar.gz logistic_regression.sav tfidf-vectorizer.sav preprocess.py inference.py requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad24ddc6",
   "metadata": {},
   "source": [
    "Now that the model is packaged, we can upload it to an Amazon S3 bucket. <br>\n",
    "We are going to use `boto3` which is an open-source Python package that allows you to easily interact with other AWS services, in our case Amazon S3 buckets to store the packaged model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65c42191",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import boto3 # Python package to interact with AWS services (S3,...)\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8f7b7b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the S3 bucket created when creating this notebook\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = sagemaker_session.default_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6268e587",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save model to the S3 bucket\n",
    "prefix = 'sentiment_analysis_lr'\n",
    "\n",
    "boto3.resource('s3').Bucket(bucket).Object(f'{prefix}/model.tar.gz').upload_file('model.tar.gz')\n",
    "model_data = f's3://{bucket}/{prefix}/model.tar.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c32ebf9-7f98-4561-9b87-fa86f1d0ea0a",
   "metadata": {},
   "source": [
    "# 5. Deploy the model\n",
    "We will now use the **SageMaker Python SDK** package to deploy our model to an API endpoint using the package's scikit-learn frameworks. <br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5bb7265-5727-4043-8ac8-dd3f6a3725af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from sagemaker.sklearn.model import SKLearnModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08388f06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!sudo chmod 777 lost+found # get the right permissions for the lost+found folder for deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d5c357",
   "metadata": {},
   "source": [
    "To deploy the model, you have to **select an instance type** that is in the same region as your session/bucket's region. <br>\n",
    "To learn more about each region's available instance types, click here: https://aws.amazon.com/fr/sagemaker/pricing/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e132ea4c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eu-west-3\n"
     ]
    }
   ],
   "source": [
    "# Check region of your notebook instance\n",
    "session = boto3.Session()\n",
    "region = session.region_name\n",
    "print(region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b738581a-86ab-4964-b3b8-e93020693626",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get IAM role created with the notebook\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# Define name of model endpoint (not mandatory)\n",
    "endpoint_name = \"sentiment-analysis-\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c010babb",
   "metadata": {},
   "source": [
    "**`SKLearnModel`** allows you to build a SageMaker model for deployment using the packaged model <br>\n",
    "Here are some of its important parameters:\n",
    "- `model_data`: The packaged model's S3 bucket location \n",
    "- `role`: The IAM role to access the S3 bucket \n",
    "- `entry_point`: Path of the inference.py file (the file that is executed as the entry point to model hosting)\n",
    "- `source_dir`: A directory that contains the preprocessing, inference and requirements scripts\n",
    "- `framework_version`: The scikit-learn package version (it should be the same as the serialized models)\n",
    "- `py_version`: Python version to execute the model code\n",
    "\n",
    "For more information, visit SageMaker Python SDK's documentation: https://sagemaker.readthedocs.io/en/stable/frameworks/sklearn/sagemaker.sklearn.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2c489c1-6a08-4461-b25b-e8e93b9e5a48",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------!"
     ]
    }
   ],
   "source": [
    "# Build SKLearn SageMaker model\n",
    "sklearn_model = SKLearnModel(\n",
    "    model_data=model_data,\n",
    "    role=role,\n",
    "    entry_point='inference.py',\n",
    "    source_dir='.',\n",
    "    framework_version='1.2-1',\n",
    "    py_version='py3'\n",
    ")\n",
    "\n",
    "# Deploy model to a SageMaker scikit-learn model server\n",
    "predictor = sklearn_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    endpoint_name=endpoint_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b310fb86-73ee-4694-8d00-9f783ad49338",
   "metadata": {},
   "source": [
    "## 6. Test the API endpoint\n",
    "We can know test the deployed model's API endpoint using `.predict()`. <br>\n",
    "We specified a JSON serializer and deserialized since SKLearnModel by default has a Numpy serializer and deserializer (which don't fit with our `output_fn` function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e4387e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['negative']\n"
     ]
    }
   ],
   "source": [
    "# Test the deployed model\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "data = {'text': \"This movie is terrible. I hated the actors and the story.\"}\n",
    "\n",
    "predictor.serializer = JSONSerializer()\n",
    "predictor.deserializer = JSONDeserializer()\n",
    "\n",
    "response = predictor.predict(json.dumps(data))\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b44d3f0a-05cd-477a-928b-df8c81ea28dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['positive']\n"
     ]
    }
   ],
   "source": [
    "data = {'text': \"This movie is amazing ! The actors and the story are great. I loved the end\"}\n",
    "\n",
    "predictor.serializer = JSONSerializer()\n",
    "predictor.deserializer = JSONDeserializer()\n",
    "\n",
    "response = predictor.predict(json.dumps(data))\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a29f2cc-0685-455c-ab05-33e020fa5ffc",
   "metadata": {},
   "source": [
    "# 7. Use the endpoint to process the results from the Slido word cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763288ba-39db-41a7-b47f-269d37682338",
   "metadata": {},
   "source": [
    "<img src=\"slido_wordcloud.png\"> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "24c401e3-ee61-445c-a599-96c1f3ee3808",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_puss_in_boots_slido = [\n",
    "    \"Amazing\",\n",
    "    \"Amazing\",\n",
    "    \"Amazing\",\n",
    "    \"Not Hot-dog\",\n",
    "    \"Not Hot-dog\",\n",
    "    \"Funny\",\n",
    "    \"Funny\",\n",
    "    \"Ohhh la la\",\n",
    "    \"Mdr\",\n",
    "    \"Miaou\",\n",
    "    \"ðŸ«‚ðŸ¦¸â€â™‚ï¸\",\n",
    "    \"Terrible\",\n",
    "    \"Awesome\",\n",
    "    \"Laught\",\n",
    "    \"Relatable\",\n",
    "    \"Sad\",\n",
    "    \"Great but too many cats\",\n",
    "    \"Bad\",\n",
    "    \"Good\",\n",
    "    \"Nice ðŸ˜Š\",\n",
    "    \"Great\",\n",
    "    \"Long live the cat\",\n",
    "    \"I like this movie, it's fun to watch and relaxing.\",\n",
    "    \"Smart\",\n",
    "    \"Cute\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5327f4a6-2af3-442d-a61b-6e5fb9dc08c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def predict_text_list(text_list, predictor):\n",
    "    \"\"\"\n",
    "    Predicts sentiment or label for a list of text inputs using a SageMaker Predictor.\n",
    "    \n",
    "    Args:\n",
    "        text_list (list of str): List of text entries to predict on.\n",
    "        predictor (Predictor): A configured SageMaker Predictor with serializer and deserializer.\n",
    "\n",
    "    Returns:\n",
    "        list: Model predictions for each input text.\n",
    "    \"\"\"\n",
    "    # Prepare data in the same format as single input\n",
    "    responses = []\n",
    "    for text in text_list:\n",
    "        data_batch = {'text': text}\n",
    "        # Set up the predictor's serializer/deserializer\n",
    "        predictor.serializer = JSONSerializer()\n",
    "        predictor.deserializer = JSONDeserializer()\n",
    "        \n",
    "        # Send to the endpoint\n",
    "        response = predictor.predict(json.dumps(data_batch))  \n",
    "        responses.append(response[0])\n",
    "    \n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "27161b51-2963-49dc-9500-7b3fc9d223aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Input: 'Amazing' --> Prediction: positive\n",
      "âœ… Input: 'Amazing' --> Prediction: positive\n",
      "âœ… Input: 'Amazing' --> Prediction: positive\n",
      "âœ… Input: 'Not Hot-dog' --> Prediction: positive\n",
      "âœ… Input: 'Not Hot-dog' --> Prediction: positive\n",
      "âœ… Input: 'Funny' --> Prediction: positive\n",
      "âœ… Input: 'Funny' --> Prediction: positive\n",
      "âœ… Input: 'Ohhh la la' --> Prediction: positive\n",
      "âœ… Input: 'Mdr' --> Prediction: positive\n",
      "âœ… Input: 'Miaou' --> Prediction: positive\n",
      "âœ… Input: 'ðŸ«‚ðŸ¦¸â€â™‚ï¸' --> Prediction: positive\n",
      "âŒ Input: 'Terrible' --> Prediction: negative\n",
      "âœ… Input: 'Awesome' --> Prediction: positive\n",
      "âœ… Input: 'Laught' --> Prediction: positive\n",
      "âœ… Input: 'Relatable' --> Prediction: positive\n",
      "âœ… Input: 'Sad' --> Prediction: positive\n",
      "âœ… Input: 'Great but too many cats' --> Prediction: positive\n",
      "âŒ Input: 'Bad' --> Prediction: negative\n",
      "âœ… Input: 'Good' --> Prediction: positive\n",
      "âœ… Input: 'Nice ðŸ˜Š' --> Prediction: positive\n",
      "âœ… Input: 'Great' --> Prediction: positive\n",
      "âœ… Input: 'Long live the cat' --> Prediction: positive\n",
      "âœ… Input: 'I like this movie, it's fun to watch and relaxing.' --> Prediction: positive\n",
      "âœ… Input: 'Smart' --> Prediction: positive\n",
      "âœ… Input: 'Cute' --> Prediction: positive\n"
     ]
    }
   ],
   "source": [
    "# Run prediction\n",
    "results = predict_text_list(data_puss_in_boots_slido, predictor)\n",
    "\n",
    "# Print results\n",
    "for text, result in zip(data_slido, results):\n",
    "    if result==\"positive\":\n",
    "        print(f\"âœ… Input: '{text}' --> Prediction: {result}\")\n",
    "    else:\n",
    "        print(f\"âŒ Input: '{text}' --> Prediction: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb8783b",
   "metadata": {},
   "source": [
    "# 8. Delete the endpoint\n",
    "\n",
    "To avoid unnecessary charges, make sure to delete the model and endpoint. <br> \n",
    "Don't forget also to stop (or delete) the notebook instance as well as delete the s3 bucket folder with the packaged model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3b405f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the model and endpoint \n",
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
